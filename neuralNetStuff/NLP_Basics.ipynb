{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKzaByut4U-W",
        "outputId": "f7df718e-20d5-44c8-9893-103ca2af6790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "nltk.download(\"brown\")\n",
        "import nltk.corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install svgling\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgNwh9vWGqPt",
        "outputId": "6f32f5ed-3ab6-4843-b913-15544df194a4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting svgling\n",
            "  Downloading svgling-0.3.1-py3-none-any.whl (21 kB)\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.3.1 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(nltk.data.find(\"corpora\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p9gWR7M4b4K",
        "outputId": "491a40ad-4599-4ac4-c3bc-312be5210617"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movie_reviews.zip', 'stopwords.zip', 'gutenberg', 'stopwords', 'wordnet_ic', 'brown.zip', 'wordnet_ic.zip', 'inaugural.zip', 'cmudict', 'gazetteers.zip', 'twitter_samples.zip', 'wordnet.zip', 'genesis.zip', 'inaugural', 'names.zip', 'words', 'shakespeare.zip', 'wordnet31.zip', 'omw.zip', 'genesis', 'treebank', 'brown', 'gutenberg.zip', 'shakespeare', 'cmudict.zip', 'names', 'omw-1.4.zip', 'treebank.zip', 'gazetteers', 'words.zip', 'wordnet2021.zip', 'movie_reviews', 'twitter_samples']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown \n",
        "brown.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po3P1aF549mB",
        "outputId": "585a9286-ee1d-4251-891d-860393d5f223"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHIqm1IF5Vp4",
        "outputId": "04295af8-94e6-4137-ec08-b26a50483b37"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "austen = nltk.corpus.gutenberg.words(\"austen-emma.txt\")"
      ],
      "metadata": {
        "id": "4vjyivSfIc1C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in austen[:500]:\n",
        "  print(word,sep = ' ', end = ' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ7aWsKBiVAO",
        "outputId": "b6e676c0-1f4d-4aa0-9583-41c1f811fee3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Emma by Jane Austen 1816 ] VOLUME I CHAPTER I Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her . She was the youngest of the two daughters of a most affectionate , indulgent father ; and had , in consequence of her sister ' s marriage , been mistress of his house from a very early period . Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses ; and her place had been supplied by an excellent woman as governess , who had fallen little short of a mother in affection . Sixteen years had Miss Taylor been in Mr . Woodhouse ' s family , less as a governess than a friend , very fond of both daughters , but particularly of Emma . Between _them_ it was more the intimacy of sisters . Even before Miss Taylor had ceased to hold the nominal office of governess , the mildness of her temper had hardly allowed her to impose any restraint ; and the shadow of authority being now long passed away , they had been living together as friend and friend very mutually attached , and Emma doing just what she liked ; highly esteeming Miss Taylor ' s judgment , but directed chiefly by her own . The real evils , indeed , of Emma ' s situation were the power of having rather too much her own way , and a disposition to think a little too well of herself ; these were the disadvantages which threatened alloy to her many enjoyments . The danger , however , was at present so unperceived , that they did not by any means rank as misfortunes with her . Sorrow came -- a gentle sorrow -- but not at all in the shape of any disagreeable consciousness .-- Miss Taylor married . It was Miss Taylor ' s loss which first brought grief . It was on the wedding - day of this beloved friend that Emma first sat in mournful thought of any continuance . The wedding over , and the bride - people gone , her father and herself were left to dine together , with no prospect of a third to cheer a long evening . Her father composed himself to sleep after dinner , as usual , and she had then only to sit and think of what she had lost . The event had every promise of happiness for her friend . Mr . Weston was a man of unexceptionable character , easy fortune , suitable age , and pleasant manners ; and there was some satisfaction in considering with what self - denying , generous friendship she had always wished and "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uk = \"\"\"What do I know about UK politics? That it is a mess! I read that the third prime minister of 2022 is serving now… who was also the previous financial minister. \n",
        "I feel like Liz Truss was absolutely not the right person. \n",
        "She put the country in a very unstable financial situation, caused by her plans for unfunded tax cuts. She seems to be chaotic and hopeless. \n",
        "\n",
        "My opinion of the UK has changed since Boris and Brexit. More since Truss, and even more since Sunak. \n",
        "I was working with UK distributors for my business and I had to change due to price increase.\n",
        "I would never have imagined this before Brexit.\"\"\"\n"
      ],
      "metadata": {
        "id": "1fRjlk_vi4Ej"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "uk_tokens = word_tokenize(uk)\n",
        "uk_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzn6hLEpi65E",
        "outputId": "c91559ad-f42e-4f09-dcce-7b9b49c309ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What',\n",
              " 'do',\n",
              " 'I',\n",
              " 'know',\n",
              " 'about',\n",
              " 'UK',\n",
              " 'politics',\n",
              " '?',\n",
              " 'That',\n",
              " 'it',\n",
              " 'is',\n",
              " 'a',\n",
              " 'mess',\n",
              " '!',\n",
              " 'I',\n",
              " 'read',\n",
              " 'that',\n",
              " 'the',\n",
              " 'third',\n",
              " 'prime',\n",
              " 'minister',\n",
              " 'of',\n",
              " '2022',\n",
              " 'is',\n",
              " 'serving',\n",
              " 'now…',\n",
              " 'who',\n",
              " 'was',\n",
              " 'also',\n",
              " 'the',\n",
              " 'previous',\n",
              " 'financial',\n",
              " 'minister',\n",
              " '.',\n",
              " 'I',\n",
              " 'feel',\n",
              " 'like',\n",
              " 'Liz',\n",
              " 'Truss',\n",
              " 'was',\n",
              " 'absolutely',\n",
              " 'not',\n",
              " 'the',\n",
              " 'right',\n",
              " 'person',\n",
              " '.',\n",
              " 'She',\n",
              " 'put',\n",
              " 'the',\n",
              " 'country',\n",
              " 'in',\n",
              " 'a',\n",
              " 'very',\n",
              " 'unstable',\n",
              " 'financial',\n",
              " 'situation',\n",
              " ',',\n",
              " 'caused',\n",
              " 'by',\n",
              " 'her',\n",
              " 'plans',\n",
              " 'for',\n",
              " 'unfunded',\n",
              " 'tax',\n",
              " 'cuts',\n",
              " '.',\n",
              " 'She',\n",
              " 'seems',\n",
              " 'to',\n",
              " 'be',\n",
              " 'chaotic',\n",
              " 'and',\n",
              " 'hopeless',\n",
              " '.',\n",
              " 'My',\n",
              " 'opinion',\n",
              " 'of',\n",
              " 'the',\n",
              " 'UK',\n",
              " 'has',\n",
              " 'changed',\n",
              " 'since',\n",
              " 'Boris',\n",
              " 'and',\n",
              " 'Brexit',\n",
              " '.',\n",
              " 'More',\n",
              " 'since',\n",
              " 'Truss',\n",
              " ',',\n",
              " 'and',\n",
              " 'even',\n",
              " 'more',\n",
              " 'since',\n",
              " 'Sunak',\n",
              " '.',\n",
              " 'I',\n",
              " 'was',\n",
              " 'working',\n",
              " 'with',\n",
              " 'UK',\n",
              " 'distributors',\n",
              " 'for',\n",
              " 'my',\n",
              " 'business',\n",
              " 'and',\n",
              " 'I',\n",
              " 'had',\n",
              " 'to',\n",
              " 'change',\n",
              " 'due',\n",
              " 'to',\n",
              " 'price',\n",
              " 'increase',\n",
              " '.',\n",
              " 'I',\n",
              " 'would',\n",
              " 'never',\n",
              " 'have',\n",
              " 'imagined',\n",
              " 'this',\n",
              " 'before',\n",
              " 'Brexit',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(uk_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gR4m_J3jgie",
        "outputId": "e6f7ca2c-5667-4f73-91db-0acd9ef92247"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()"
      ],
      "metadata": {
        "id": "Zji9mLRiji3B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in uk_tokens:\n",
        "  fdist[word.lower()]+=1\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwuImCGvjxpp",
        "outputId": "47b35135-fe7d-44a9-eac4-217082e809ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'.': 8, 'i': 6, 'the': 5, 'and': 4, 'uk': 3, 'was': 3, 'to': 3, 'since': 3, 'that': 2, 'is': 2, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist['politics']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHnmjyLdkA_r",
        "outputId": "d34b4270-46dc-45c2-9b7a-5d256b7df445"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(fdist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAxMbPRokFCR",
        "outputId": "47d372af-031c-4ec4-c27f-bc775cea1714"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_top10 = fdist.most_common(10)\n",
        "fdist_top10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvD4FM3gkLEF",
        "outputId": "b7de235c-736d-405c-a433-366d3b2f2d30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 8),\n",
              " ('i', 6),\n",
              " ('the', 5),\n",
              " ('and', 4),\n",
              " ('uk', 3),\n",
              " ('was', 3),\n",
              " ('to', 3),\n",
              " ('since', 3),\n",
              " ('that', 2),\n",
              " ('is', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import blankline_tokenize\n",
        "uk_blank = blankline_tokenize(uk)\n",
        "len(uk_blank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GrpvBNJkT10",
        "outputId": "c06764be-d49b-4633-cd63-101563b160ed"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams\n",
        "string = \"Le ball is le ball, because le ball is le ball. Indeed. Balls\"\n",
        "quotes_tokens = nltk.word_tokenize(string)\n",
        "quotes_tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUuUkMCkkmFP",
        "outputId": "17f150bf-9435-4a36-c049-e55bd10c1b64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Le',\n",
              " 'ball',\n",
              " 'is',\n",
              " 'le',\n",
              " 'ball',\n",
              " ',',\n",
              " 'because',\n",
              " 'le',\n",
              " 'ball',\n",
              " 'is',\n",
              " 'le',\n",
              " 'ball',\n",
              " '.',\n",
              " 'Indeed',\n",
              " '.',\n",
              " 'Balls']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
        "quotes_bigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoqhXKwVlTJm",
        "outputId": "db1b64a2-b539-44fc-9505-e36db814c187"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Le', 'ball'),\n",
              " ('ball', 'is'),\n",
              " ('is', 'le'),\n",
              " ('le', 'ball'),\n",
              " ('ball', ','),\n",
              " (',', 'because'),\n",
              " ('because', 'le'),\n",
              " ('le', 'ball'),\n",
              " ('ball', 'is'),\n",
              " ('is', 'le'),\n",
              " ('le', 'ball'),\n",
              " ('ball', '.'),\n",
              " ('.', 'Indeed'),\n",
              " ('Indeed', '.'),\n",
              " ('.', 'Balls')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
        "quotes_trigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78GuAMJjlnLP",
        "outputId": "a98a67a8-655e-446c-bf20-78e9265d3750"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Le', 'ball', 'is'),\n",
              " ('ball', 'is', 'le'),\n",
              " ('is', 'le', 'ball'),\n",
              " ('le', 'ball', ','),\n",
              " ('ball', ',', 'because'),\n",
              " (',', 'because', 'le'),\n",
              " ('because', 'le', 'ball'),\n",
              " ('le', 'ball', 'is'),\n",
              " ('ball', 'is', 'le'),\n",
              " ('is', 'le', 'ball'),\n",
              " ('le', 'ball', '.'),\n",
              " ('ball', '.', 'Indeed'),\n",
              " ('.', 'Indeed', '.'),\n",
              " ('Indeed', '.', 'Balls')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_ngrams = list(nltk.ngrams(quotes_tokens, 5))\n",
        "quotes_ngrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPzramXUlrfO",
        "outputId": "347e09fb-71bc-4dcd-eec5-e3ecfdcedcf8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Le', 'ball', 'is', 'le', 'ball'),\n",
              " ('ball', 'is', 'le', 'ball', ','),\n",
              " ('is', 'le', 'ball', ',', 'because'),\n",
              " ('le', 'ball', ',', 'because', 'le'),\n",
              " ('ball', ',', 'because', 'le', 'ball'),\n",
              " (',', 'because', 'le', 'ball', 'is'),\n",
              " ('because', 'le', 'ball', 'is', 'le'),\n",
              " ('le', 'ball', 'is', 'le', 'ball'),\n",
              " ('ball', 'is', 'le', 'ball', '.'),\n",
              " ('is', 'le', 'ball', '.', 'Indeed'),\n",
              " ('le', 'ball', '.', 'Indeed', '.'),\n",
              " ('ball', '.', 'Indeed', '.', 'Balls')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()\n",
        "\n",
        "pst.stem(\"having\")"
      ],
      "metadata": {
        "id": "mATlt2kQ6MEl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bf5f789c-e339-4724-864d-310259455b06"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_stem = [\"give\", \"giving\", \"given\", \"gave\"]\n",
        "\n",
        "for words in words_to_stem:\n",
        "  print(words+ \" : \" +pst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wbv_Jb-_Lat",
        "outputId": "56fb4369-e470-4934-a627-ecd2259f65a7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give : give\n",
            "giving : give\n",
            "given : given\n",
            "gave : gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lst = LancasterStemmer()\n",
        "for words in words_to_stem:\n",
        "  print(words+ \" : \" + lst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDxj-Vu7_aTh",
        "outputId": "c181ffe5-e823-43ff-a6ba-84fcda477f6d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give : giv\n",
            "giving : giv\n",
            "given : giv\n",
            "gave : gav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "sbst = SnowballStemmer('english')\n",
        "\n",
        "for words in words_to_stem:\n",
        "  print(words + \" : \" + sbst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5WaSJqu_0xw",
        "outputId": "e950578f-015f-4ed4-c777-707ef34c43c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give : give\n",
            "giving : give\n",
            "given : given\n",
            "gave : gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_lem = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "8eCmNippAa0Q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_lem.lemmatize(\"corpora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gm5qOu9nA5D5",
        "outputId": "5fef99a8-e656-4595-d5ee-e0afeb735b25"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'corpus'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in words_to_stem:\n",
        "  print(words + \" : \"+ word_lem.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9GgURiNA8ne",
        "outputId": "8e45bd8d-3bae-48f4-f656-308dab91e1fe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give : give\n",
            "giving : giving\n",
            "given : given\n",
            "gave : gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGDESLrBBD5-",
        "outputId": "b3203ed4-5872-41e3-eeb5-3148825116a6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4zGhgsGBVKs",
        "outputId": "7c0f9df5-8f14-4fbb-efe4-1c75bef06b3b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "punctuation = re.compile(r'[-.?!,:;()|0-9]')"
      ],
      "metadata": {
        "id": "6YZHLkUeBY1Q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_punctuation = [] \n",
        "\n",
        "for words in uk_tokens:\n",
        "  word = punctuation.sub(\"\", words)\n",
        "  if len(word) > 0:\n",
        "    post_punctuation.append(word)\n",
        "\n",
        "post_punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyAbl08iBnmk",
        "outputId": "bffe84c9-0fde-4af8-b9d3-c08940483727"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What',\n",
              " 'do',\n",
              " 'I',\n",
              " 'know',\n",
              " 'about',\n",
              " 'UK',\n",
              " 'politics',\n",
              " 'That',\n",
              " 'it',\n",
              " 'is',\n",
              " 'a',\n",
              " 'mess',\n",
              " 'I',\n",
              " 'read',\n",
              " 'that',\n",
              " 'the',\n",
              " 'third',\n",
              " 'prime',\n",
              " 'minister',\n",
              " 'of',\n",
              " 'is',\n",
              " 'serving',\n",
              " 'now…',\n",
              " 'who',\n",
              " 'was',\n",
              " 'also',\n",
              " 'the',\n",
              " 'previous',\n",
              " 'financial',\n",
              " 'minister',\n",
              " 'I',\n",
              " 'feel',\n",
              " 'like',\n",
              " 'Liz',\n",
              " 'Truss',\n",
              " 'was',\n",
              " 'absolutely',\n",
              " 'not',\n",
              " 'the',\n",
              " 'right',\n",
              " 'person',\n",
              " 'She',\n",
              " 'put',\n",
              " 'the',\n",
              " 'country',\n",
              " 'in',\n",
              " 'a',\n",
              " 'very',\n",
              " 'unstable',\n",
              " 'financial',\n",
              " 'situation',\n",
              " 'caused',\n",
              " 'by',\n",
              " 'her',\n",
              " 'plans',\n",
              " 'for',\n",
              " 'unfunded',\n",
              " 'tax',\n",
              " 'cuts',\n",
              " 'She',\n",
              " 'seems',\n",
              " 'to',\n",
              " 'be',\n",
              " 'chaotic',\n",
              " 'and',\n",
              " 'hopeless',\n",
              " 'My',\n",
              " 'opinion',\n",
              " 'of',\n",
              " 'the',\n",
              " 'UK',\n",
              " 'has',\n",
              " 'changed',\n",
              " 'since',\n",
              " 'Boris',\n",
              " 'and',\n",
              " 'Brexit',\n",
              " 'More',\n",
              " 'since',\n",
              " 'Truss',\n",
              " 'and',\n",
              " 'even',\n",
              " 'more',\n",
              " 'since',\n",
              " 'Sunak',\n",
              " 'I',\n",
              " 'was',\n",
              " 'working',\n",
              " 'with',\n",
              " 'UK',\n",
              " 'distributors',\n",
              " 'for',\n",
              " 'my',\n",
              " 'business',\n",
              " 'and',\n",
              " 'I',\n",
              " 'had',\n",
              " 'to',\n",
              " 'change',\n",
              " 'due',\n",
              " 'to',\n",
              " 'price',\n",
              " 'increase',\n",
              " 'I',\n",
              " 'would',\n",
              " 'never',\n",
              " 'have',\n",
              " 'imagined',\n",
              " 'this',\n",
              " 'before',\n",
              " 'Brexit']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"Everyone else is a natural when it comes to maths\"\n",
        "sent_tokens = word_tokenize(sent)"
      ],
      "metadata": {
        "id": "Pd2ze2mECjEP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in sent_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANGJhcg6Cyeu",
        "outputId": "b7090fd1-9c27-4f9a-94da-05b0affd5c40"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Everyone', 'NN')]\n",
            "[('else', 'RB')]\n",
            "[('is', 'VBZ')]\n",
            "[('a', 'DT')]\n",
            "[('natural', 'JJ')]\n",
            "[('when', 'WRB')]\n",
            "[('it', 'PRP')]\n",
            "[('comes', 'VBZ')]\n",
            "[('to', 'TO')]\n",
            "[('maths', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example of shortcoming of pos tagging method (2nd and 3rd tokens)\n",
        "sent2 = \"He is eating the extension cord\"\n",
        "sent2_tokens = word_tokenize(sent2)\n",
        "\n",
        "for token in sent2_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgRCMbglC2ym",
        "outputId": "e0b177fe-a515-454b-b16f-0b8a39567cff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('He', 'PRP')]\n",
            "[('is', 'VBZ')]\n",
            "[('eating', 'VBG')]\n",
            "[('the', 'DT')]\n",
            "[('extension', 'NN')]\n",
            "[('cord', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "NE_sent = \"The former PM lost to a lettuce.\"\n",
        "NE_tokens = word_tokenize(NE_sent)\n",
        "NE_tags = nltk.pos_tag(NE_tokens)"
      ],
      "metadata": {
        "id": "izZfio2YDeyw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ORGANIZATION misclassed\n",
        "NE_NER = ne_chunk(NE_tags)\n",
        "print(NE_NER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KecfQZv2Dwu2",
        "outputId": "c69be64e-3b88-4a63-9c8a-5b01541c85b2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  former/JJ\n",
            "  (ORGANIZATION PM/NNP)\n",
            "  lost/VBD\n",
            "  to/TO\n",
            "  a/DT\n",
            "  lettuce/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chonking time\n",
        "new = \"The gigantic giant died after a little mouse bit his foot\"\n",
        "new_tokens = nltk.pos_tag(word_tokenize(new))\n",
        "new_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUlU2oWmD7de",
        "outputId": "8f3e82d9-6fd4-4636-ba51-cbe8ef4021fd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('gigantic', 'JJ'),\n",
              " ('giant', 'NN'),\n",
              " ('died', 'VBD'),\n",
              " ('after', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('little', 'JJ'),\n",
              " ('mouse', 'NN'),\n",
              " ('bit', 'VBD'),\n",
              " ('his', 'PRP$'),\n",
              " ('foot', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar_np = r\"NP: {<DT>?<JJ>*<NN>}\""
      ],
      "metadata": {
        "id": "nRgC3P1mFGIr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_parser = nltk.RegexpParser(grammar_np)"
      ],
      "metadata": {
        "id": "dmA7hgZXFQxX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_result = chunk_parser.parse(new_tokens)\n",
        "chunk_result "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "PfQSmAOLFVLs",
        "outputId": "9f9748e0-2aca-4217-ad13-87329da9b006"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('The', 'DT'), ('gigantic', 'JJ'), ('giant', 'NN')]), ('died', 'VBD'), ('after', 'IN'), Tree('NP', [('a', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('bit', 'VBD'), ('his', 'PRP$'), Tree('NP', [('foot', 'NN')])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,568.0,168.0\" width=\"568px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"30.9859%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"22.7273%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.3636%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"45.4545%\" x=\"22.7273%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">gigantic</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.4545%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"31.8182%\" x=\"68.1818%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">giant</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.0909%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.493%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.4507%\" x=\"30.9859%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">died</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35.2113%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.85915%\" x=\"39.4366%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">after</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"44.3662%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"26.7606%\" x=\"49.2958%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"21.0526%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.5263%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"42.1053%\" x=\"21.0526%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">little</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.1053%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"36.8421%\" x=\"63.1579%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">mouse</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.5789%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.6761%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.04225%\" x=\"76.0563%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">bit</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.5775%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.4507%\" x=\"83.0986%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">his</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"87.3239%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.4507%\" x=\"91.5493%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">foot</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.7746%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SJPMVAQPGVBD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}